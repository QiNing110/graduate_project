{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ffe5668d-7783-43a0-8742-563c423489d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/envs/MateConv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import ast\n",
    "import csv\n",
    "\n",
    "import pandas as pd\n",
    "from modelscope import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "890c1d9f-0fde-4bad-8bfa-203c9252342a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/root/autodl-tmp'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f5054705-e326-4582-b7f5-a79c0b7ea6c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['MODELSCOPE_CACHE'] =  './models'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c0122251-9e27-43d3-9252-21dd7027d23d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(file_path):\n",
    "    df = pd.read_csv(file_path)\n",
    "    texts = df['text'].tolist()\n",
    "    labels = []\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        reader = csv.DictReader(file)\n",
    "        # 遍历每一行数据\n",
    "        for row in reader:\n",
    "            # 获取 info_list 列的值\n",
    "            info_list_str = row['info_list']\n",
    "            # 如果 info_list 不为空字符串\n",
    "            if info_list_str:\n",
    "                try:\n",
    "                    # 将字符串解析为 Python 对象\n",
    "                    info_list = ast.literal_eval(info_list_str)\n",
    "                    # 初始化一个空列表，用于存储解析后的字典\n",
    "                    parsed_info = []\n",
    "                    # 遍历 info_list 中的每个子列表\n",
    "                    for sub_list in info_list:\n",
    "                        for item in sub_list:\n",
    "                            # 提取 type 和 span 信息\n",
    "                            parsed_info.append({\n",
    "                                'type': item['type'],\n",
    "                                'span': item['span']\n",
    "                            })\n",
    "                    # 将解析后的信息添加到结果列表中\n",
    "                    row['info_list'] = parsed_info\n",
    "                except (SyntaxError, ValueError):\n",
    "                    print(f\"无法解析行 {row['id']} 的 info_list 列\")\n",
    "            else:\n",
    "                # 如果 info_list 为空字符串，将其设置为空列表\n",
    "                row['info_list'] = []\n",
    "            # 将处理后的行添加到解析结果列表中\n",
    "            labels.append(row['info_list'])\n",
    "    return texts, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "aa85ea0d-fb44-4ca5-b87e-51d9c6a40730",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(model_name):\n",
    "    cache_dir = 'root/autodl-tmp/models'\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name, cache_dir = cache_dir,trust_remote_code=False)\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_name,\n",
    "                                                 device_map='auto',\n",
    "                                                 torch_dtype=\"auto\",\n",
    "                                                 cache_dir=cache_dir,\n",
    "                                                 trust_remote_code=True)\n",
    "    return tokenizer, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3b2c94d6-812b-4809-aa02-af3078ea1d43",
   "metadata": {},
   "outputs": [],
   "source": [
    "from modelscope import GenerationConfig\n",
    "\n",
    "def inference(tokenizer, model, text):\n",
    "    content = f'''\n",
    "请对以下文本进行实体识别，并按照特定格式输出结果。\n",
    "示例：\n",
    "文本: \"相比之下，青岛海牛队和广州松日队的雨中之战虽然也是0∶0，但乏善可陈。\"\n",
    "输出: [{{\"span\": \"广州松日队\", \"type\": \"组织机构\"}}, {{\"span\": \"青岛海牛队\", \"type\": \"组织机构\"}}]。\n",
    "\n",
    "请使用如下格式输出识别结果：\n",
    "[\n",
    "    {{\"span\": \"实体在文本中的具体内容\", \"type\": \"预定义的实体类型，如人物、组织机构、地理位置等\"}}\n",
    "]\n",
    "\n",
    "如果文本中没有识别出任何实体，请输出 []。只能输出最终的格式化结果，不能输出其他任何额外解释。\n",
    "实体类型包括：人物、地理位置、组织机构。\n",
    "文本: {text}\n",
    "输出:\n",
    "'''\n",
    "    model.generation_config = GenerationConfig.from_pretrained('./models/models/deepseek-ai/deepseek-moe-16b-chat')\n",
    "    model.generation_config.pad_token_id = model.generation_config.eos_token_id\n",
    "    \n",
    "    \n",
    "    messages = [\n",
    "        # {\"role\": \"system\", \"content\": \"You are Qwen, created by Alibaba Cloud. You are a helpful assistant.\"},\n",
    "        {\"role\": \"user\", \"content\": content}\n",
    "    ]\n",
    "    input_tensor = tokenizer.apply_chat_template(messages, add_generation_prompt=True, return_tensors=\"pt\")\n",
    "    outputs = model.generate(input_tensor.to(model.device), \n",
    "                             max_new_tokens=100\n",
    "                            )\n",
    "    \n",
    "    result = tokenizer.decode(outputs[0][input_tensor.shape[1]:], skip_special_tokens=True)\n",
    "\n",
    "    # index = response.find(\"</think>\\n\")\n",
    "\n",
    "    try:\n",
    "        import ast\n",
    "        entities = ast.literal_eval(result)\n",
    "    except:\n",
    "        print(\"解析为列表失败\")\n",
    "        entities = []\n",
    "    return entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "22fa6a9f-6c1b-4d18-904c-1762b6e6d550",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(labels, predictions):\n",
    "    all_true_labels = []\n",
    "    all_pred_labels = []\n",
    "    for true_entities, pred_entities in zip(labels, predictions):\n",
    "        true_span_types = [span_type[\"span\"] for span_type in true_entities]\n",
    "        pred_span_types = [span_type[\"span\"] for span_type in pred_entities]\n",
    "        all_true_labels.append(true_span_types)\n",
    "        all_pred_labels.append(pred_span_types)\n",
    "    # 初始化 MultiLabelBinarizer\n",
    "    mlb = MultiLabelBinarizer()\n",
    "\n",
    "    # 对真实标签进行拟合和转换\n",
    "    y_true = mlb.fit_transform(all_true_labels)\n",
    "\n",
    "    # 对预测标签进行转换（使用在真实标签上拟合的编码器）\n",
    "    y_pred = mlb.transform(all_pred_labels)\n",
    "    precision = precision_score(y_true, y_pred, average='weighted')\n",
    "    recall = recall_score(y_true, y_pred, average='weighted')\n",
    "    f1 = f1_score(y_true, y_pred, average='weighted')\n",
    "    return precision, recall, f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "96fdadfd-dba7-43a1-b760-5042575eb5e0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-07 12:49:08,903 - modelscope - WARNING - Using branch: master as version is unstable, use with caution\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading Model from https://www.modelscope.cn to directory: ./models/models/deepseek-ai/deepseek-moe-16b-chat\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "2025-05-07 12:49:09,843 - modelscope - WARNING - Using branch: master as version is unstable, use with caution\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading Model from https://www.modelscope.cn to directory: ./models/models/deepseek-ai/deepseek-moe-16b-chat\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "This modeling file requires the following packages that were not found in your environment: flash_attn. Run `pip install flash_attn`",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mImportError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 12\u001b[39m\n\u001b[32m      3\u001b[39m model_names = [\n\u001b[32m      4\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mQwen/Qwen2.5-14B-Instruct\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m      5\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mQwen/Qwen3-14B\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m      8\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mbaichuan-inc/Baichuan2-13B-Chat\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m      9\u001b[39m ]\n\u001b[32m     10\u001b[39m model_name = \u001b[33m'\u001b[39m\u001b[33mdeepseek-ai/deepseek-moe-16b-chat\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m12\u001b[39m tokenizer, model = \u001b[43mload_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 4\u001b[39m, in \u001b[36mload_model\u001b[39m\u001b[34m(model_name)\u001b[39m\n\u001b[32m      2\u001b[39m cache_dir = \u001b[33m'\u001b[39m\u001b[33mroot/autodl-tmp/models\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m      3\u001b[39m tokenizer = AutoTokenizer.from_pretrained(model_name, cache_dir = cache_dir,trust_remote_code=\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m model = \u001b[43mAutoModelForCausalLM\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      5\u001b[39m \u001b[43m                                             \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mauto\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      6\u001b[39m \u001b[43m                                             \u001b[49m\u001b[43mtorch_dtype\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mauto\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      7\u001b[39m \u001b[43m                                             \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      8\u001b[39m \u001b[43m                                             \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m      9\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m tokenizer, model\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/MateConv/lib/python3.11/site-packages/modelscope/utils/hf_util/patcher.py:247\u001b[39m, in \u001b[36m_patch_pretrained_class.<locals>.get_wrapped_class.<locals>.ClassWrapper.from_pretrained\u001b[39m\u001b[34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[39m\n\u001b[32m    238\u001b[39m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[32m    239\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mfrom_pretrained\u001b[39m(\u001b[38;5;28mcls\u001b[39m, pretrained_model_name_or_path,\n\u001b[32m    240\u001b[39m                     *model_args, **kwargs):\n\u001b[32m    241\u001b[39m     model_dir = get_model_dir(\n\u001b[32m    242\u001b[39m         pretrained_model_name_or_path,\n\u001b[32m    243\u001b[39m         ignore_file_pattern=ignore_file_pattern,\n\u001b[32m    244\u001b[39m         allow_file_pattern=allow_file_pattern,\n\u001b[32m    245\u001b[39m         **kwargs)\n\u001b[32m--> \u001b[39m\u001b[32m247\u001b[39m     module_obj = \u001b[43mmodule_class\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    248\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmodel_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43mmodel_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    250\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m module_class.\u001b[34m__name__\u001b[39m.startswith(\u001b[33m'\u001b[39m\u001b[33mAutoModel\u001b[39m\u001b[33m'\u001b[39m):\n\u001b[32m    251\u001b[39m         module_obj.model_dir = model_dir\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/MateConv/lib/python3.11/site-packages/transformers/models/auto/auto_factory.py:550\u001b[39m, in \u001b[36m_BaseAutoModelClass.from_pretrained\u001b[39m\u001b[34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[39m\n\u001b[32m    548\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m has_remote_code \u001b[38;5;129;01mand\u001b[39;00m trust_remote_code:\n\u001b[32m    549\u001b[39m     class_ref = config.auto_map[\u001b[38;5;28mcls\u001b[39m.\u001b[34m__name__\u001b[39m]\n\u001b[32m--> \u001b[39m\u001b[32m550\u001b[39m     model_class = \u001b[43mget_class_from_dynamic_module\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    551\u001b[39m \u001b[43m        \u001b[49m\u001b[43mclass_ref\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcode_revision\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcode_revision\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mhub_kwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    552\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    553\u001b[39m     _ = hub_kwargs.pop(\u001b[33m\"\u001b[39m\u001b[33mcode_revision\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m    554\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m os.path.isdir(pretrained_model_name_or_path):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/MateConv/lib/python3.11/site-packages/transformers/dynamic_module_utils.py:489\u001b[39m, in \u001b[36mget_class_from_dynamic_module\u001b[39m\u001b[34m(class_reference, pretrained_model_name_or_path, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, repo_type, code_revision, **kwargs)\u001b[39m\n\u001b[32m    487\u001b[39m     code_revision = revision\n\u001b[32m    488\u001b[39m \u001b[38;5;66;03m# And lastly we get the class inside our newly created module\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m489\u001b[39m final_module = \u001b[43mget_cached_module_file\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    490\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrepo_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    491\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodule_file\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m.py\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    492\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    493\u001b[39m \u001b[43m    \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[43m=\u001b[49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    494\u001b[39m \u001b[43m    \u001b[49m\u001b[43mresume_download\u001b[49m\u001b[43m=\u001b[49m\u001b[43mresume_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    495\u001b[39m \u001b[43m    \u001b[49m\u001b[43mproxies\u001b[49m\u001b[43m=\u001b[49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    496\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    497\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcode_revision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    498\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    499\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    500\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    501\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m get_class_in_module(class_name, final_module)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/MateConv/lib/python3.11/site-packages/transformers/dynamic_module_utils.py:315\u001b[39m, in \u001b[36mget_cached_module_file\u001b[39m\u001b[34m(pretrained_model_name_or_path, module_file, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, repo_type, _commit_hash, **deprecated_kwargs)\u001b[39m\n\u001b[32m    312\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[32m    314\u001b[39m \u001b[38;5;66;03m# Check we have all the requirements in our environment\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m315\u001b[39m modules_needed = \u001b[43mcheck_imports\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresolved_module_file\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    317\u001b[39m \u001b[38;5;66;03m# Now we move the module inside our cached dynamic modules.\u001b[39;00m\n\u001b[32m    318\u001b[39m full_submodule = TRANSFORMERS_DYNAMIC_MODULE_NAME + os.path.sep + submodule\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/MateConv/lib/python3.11/site-packages/transformers/dynamic_module_utils.py:180\u001b[39m, in \u001b[36mcheck_imports\u001b[39m\u001b[34m(filename)\u001b[39m\n\u001b[32m    177\u001b[39m         missing_packages.append(imp)\n\u001b[32m    179\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(missing_packages) > \u001b[32m0\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m180\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\n\u001b[32m    181\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mThis modeling file requires the following packages that were not found in your environment: \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    182\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m, \u001b[39m\u001b[33m'\u001b[39m.join(missing_packages)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m. Run `pip install \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m \u001b[39m\u001b[33m'\u001b[39m.join(missing_packages)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m`\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    183\u001b[39m     )\n\u001b[32m    185\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m get_relative_imports(filename)\n",
      "\u001b[31mImportError\u001b[39m: This modeling file requires the following packages that were not found in your environment: flash_attn. Run `pip install flash_attn`"
     ]
    }
   ],
   "source": [
    "file_path = 'dev.csv'\n",
    "texts, labels = load_data(file_path)\n",
    "model_names = [\n",
    "    'Qwen/Qwen2.5-14B-Instruct',\n",
    "    'Qwen/Qwen3-14B',\n",
    "    'deepseek-ai/DeepSeek-R1-Distill-Qwen-14B',\n",
    "    'deepseek-ai/deepseek-moe-16b-chat',\n",
    "    'baichuan-inc/Baichuan2-13B-Chat'\n",
    "]\n",
    "model_name = 'deepseek-ai/deepseek-moe-16b-chat'\n",
    "\n",
    "tokenizer, model = load_model(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07093660-6cbc-4c26-bf04-95039494a274",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = []\n",
    "for text in tqdm(texts):\n",
    "    entities = inference(tokenizer, model, text)\n",
    "    print(entities)\n",
    "    predictions.append(entities)\n",
    "print(predictions)\n",
    "precision, recall, f1 = evaluate(labels, predictions)\n",
    "print(f\"Model: {model_name}\")\n",
    "print(f\"Precision: {precision}\")\n",
    "print(f\"Recall: {recall}\")\n",
    "print(f\"F1-score: {f1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aa7025a-550d-4408-b1ec-4c9872ded4d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    file_path = 'dev.csv'\n",
    "    texts, labels = load_data(file_path)\n",
    "    model_names = [\n",
    "        'Qwen/Qwen2.5-14B-Instruct',\n",
    "        'Qwen/Qwen3-14B',\n",
    "        'deepseek-ai/DeepSeek-R1-Distill-Qwen-14B',\n",
    "        'deepseek-ai/deepseek-moe-16b-chat',\n",
    "        'baichuan-inc/Baichuan2-13B-Chat'\n",
    "    ]\n",
    "    model_name = 'deepseek-ai/deepseek-moe-16b-chat'\n",
    "\n",
    "    tokenizer, model = load_model(model_name)\n",
    "    predictions = []\n",
    "    for text in tqdm(texts):\n",
    "        entities = inference(tokenizer, model, text)\n",
    "        print(entities)\n",
    "        predictions.append(entities)\n",
    "    print(predictions)\n",
    "    precision, recall, f1 = evaluate(labels, predictions)\n",
    "    print(f\"Model: {model_name}\")\n",
    "    print(f\"Precision: {precision}\")\n",
    "    print(f\"Recall: {recall}\")\n",
    "    print(f\"F1-score: {f1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "507f5374-8dc5-44c8-9d50-0b0266f9c6a4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (MateConv)",
   "language": "python",
   "name": "mateconv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
